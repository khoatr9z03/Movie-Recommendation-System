{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Notebook - Movie Recommendation System\n",
    "\n",
    "**Purpose:** Generate `recommendations_top50.parquet` and verify all data files are ready.\n",
    "\n",
    "**Run this notebook ONCE before running the Streamlit app.**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Install Dependencies](#1.-Install-Dependencies)\n",
    "2. [Import Libraries](#2.-Import-Libraries)\n",
    "3. [Setup Paths](#3.-Setup-Paths)\n",
    "4. [Verify Data Files](#4.-Verify-Data-Files)\n",
    "5. [Generate Recommendations Parquet](#5.-Generate-Recommendations-Parquet)\n",
    "6. [Test Data Loading](#6.-Test-Data-Loading)\n",
    "7. [Run Streamlit App](#7.-Run-Streamlit-App)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install required packages for local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below if packages are not installed\n",
    "\n",
    "# !pip install streamlit pandas numpy pyarrow requests python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import all necessary libraries for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"● All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Paths\n",
    "\n",
    "Define paths to data directory and files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● Configuration:\n",
      "  Data directory: tmdb_dataset\n",
      "  Top-K recommendations: 50\n",
      "  Output file: tmdb_dataset\\recommendations_top50.parquet\n",
      "\n",
      "● Paths configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = \"tmdb_dataset\"  # Relative path to data directory\n",
    "TOP_K = 50  # Number of recommendations to pre-compute per movie\n",
    "\n",
    "# File paths\n",
    "SIMILARITY_MATRIX_FILE = os.path.join(DATA_DIR, \"similarity_matrix.npy\")\n",
    "MOVIE_INDICES_FILE = os.path.join(DATA_DIR, \"movie_indices.csv\")\n",
    "MOVIES_FINAL_FILE = os.path.join(DATA_DIR, \"movies_final.csv\")\n",
    "OUTPUT_FILE = os.path.join(DATA_DIR, \"recommendations_top50.parquet\")\n",
    "\n",
    "# Display configuration\n",
    "print(\"● Configuration:\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Top-K recommendations: {TOP_K}\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")\n",
    "print()\n",
    "print(f\"● Paths configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Data Files\n",
    "\n",
    "Check that all required files exist before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● Verifying required files...\n",
      "================================================================================\n",
      "Similarity Matrix   :   Found (2,023.57 MB)\n",
      "                      Path: tmdb_dataset\\similarity_matrix.npy\n",
      "\n",
      "Movie Indices       :   Found (0.45 MB)\n",
      "                      Path: tmdb_dataset\\movie_indices.csv\n",
      "\n",
      "Movies Final        :   Found (19.37 MB)\n",
      "                      Path: tmdb_dataset\\movies_final.csv\n",
      "\n",
      "================================================================================\n",
      "● All required files found! Ready to proceed.\n"
     ]
    }
   ],
   "source": [
    "print(\"● Verifying required files...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "required_files = {\n",
    "    \"Similarity Matrix\": SIMILARITY_MATRIX_FILE,\n",
    "    \"Movie Indices\": MOVIE_INDICES_FILE,\n",
    "    \"Movies Final\": MOVIES_FINAL_FILE\n",
    "}\n",
    "\n",
    "all_exist = True\n",
    "\n",
    "for name, filepath in required_files.items():\n",
    "    exists = os.path.exists(filepath)\n",
    "    \n",
    "    if exists:\n",
    "        # Get file size\n",
    "        size_mb = os.path.getsize(filepath) / (1024 ** 2)\n",
    "        status = f\"  Found ({size_mb:,.2f} MB)\"\n",
    "    else:\n",
    "        status = \"  MISSING\"\n",
    "        all_exist = False\n",
    "    \n",
    "    print(f\"{name:20s}: {status}\")\n",
    "    print(f\"{'':20s}  Path: {filepath}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if all_exist:\n",
    "    print(\"● All required files found! Ready to proceed.\")\n",
    "else:\n",
    "    print(\"● Some files are missing!\")\n",
    "    print(\"  Please make sure to:\")\n",
    "    print(\"  1. Run the complete MovieRecSys.ipynb pipeline first\")\n",
    "    print(\"  2. Copy all generated files to the tmdb_dataset/ folder\")\n",
    "    raise FileNotFoundError(\"Missing required data files. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Recommendations Parquet\n",
    "\n",
    "Generate pre-computed recommendations from similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmdb_dataset\\recommendations_top50.parquet already exists!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to regenerate? (yes/no):  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Skipping generation. Using existing file.\n",
      "\n",
      "\n",
      "Recommendations file already exists. Skipping generation.\n"
     ]
    }
   ],
   "source": [
    "# Check if parquet file already exists\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    print(f\"{OUTPUT_FILE} already exists!\")\n",
    "    print()\n",
    "\n",
    "    # Ask user if they want to regenerate\n",
    "    regenerate = input(\"Do you want to regenerate? (yes/no): \").strip().lower()\n",
    "\n",
    "    if regenerate == 'yes':\n",
    "        os.remove(OUTPUT_FILE)\n",
    "        print(f\"  Deleted old {OUTPUT_FILE}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(\"  Skipping generation. Using existing file.\")\n",
    "        print()\n",
    "\n",
    "# Generate if file doesn't exist\n",
    "if not os.path.exists(OUTPUT_FILE):\n",
    "    print(\"\\nGENERATING RECOMMENDATIONS PARQUET FILE\")\n",
    "    print()\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    print(\"[1/4] Loading data files...\")\n",
    "    similarity_matrix = np.load(SIMILARITY_MATRIX_FILE)\n",
    "    print(f\"  Loaded {SIMILARITY_MATRIX_FILE}\")\n",
    "    print(f\"  Shape: {similarity_matrix.shape}\")\n",
    "    print(f\"  Size: {similarity_matrix.nbytes / 1024**2:.2f} MB\")\n",
    "    print()\n",
    "    \n",
    "    movie_indices = pd.read_csv(MOVIE_INDICES_FILE)\n",
    "    print(f\"  Loaded {MOVIE_INDICES_FILE}\")\n",
    "    print(f\"  Records: {len(movie_indices):,}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Validate data\n",
    "    print(\"[2/4] Validating data...\")\n",
    "    n_movies = len(similarity_matrix)\n",
    "    \n",
    "    assert similarity_matrix.shape[0] == similarity_matrix.shape[1], \"Similarity matrix must be square!\"\n",
    "    assert len(movie_indices) == n_movies, f\"Index length mismatch! Matrix: {n_movies}, Indices: {len(movie_indices)}\"\n",
    "    \n",
    "    print(f\"  Data validated: {n_movies:,} movies\")\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Generate recommendations\n",
    "    print(f\"[3/4] Generating top-{TOP_K} recommendations for each movie...\")\n",
    "    print(f\"  This may take 1-3 minutes...\")\n",
    "    print()\n",
    "    \n",
    "    recommendations_data = []\n",
    "    \n",
    "    for idx in tqdm(range(n_movies), desc=\"   Processing\"):\n",
    "        # Get similarity scores for this movie\n",
    "        sim_scores = similarity_matrix[idx]\n",
    "        \n",
    "        # Get top-K indices (excluding itself)\n",
    "        top_indices = np.argsort(sim_scores)[::-1][1:TOP_K+1]\n",
    "        top_scores = sim_scores[top_indices]\n",
    "        \n",
    "        # Get movie IDs\n",
    "        movie_id = movie_indices.iloc[idx]['movie_id']\n",
    "        recommended_ids = movie_indices.iloc[top_indices]['movie_id'].values\n",
    "        \n",
    "        # Store recommendations\n",
    "        for rank, (rec_id, score) in enumerate(zip(recommended_ids, top_scores), 1):\n",
    "            recommendations_data.append({\n",
    "                'movie_id': int(movie_id),\n",
    "                'rank': rank,\n",
    "                'recommended_id': int(rec_id),\n",
    "                'similarity': float(score)\n",
    "            })\n",
    "    \n",
    "    print()\n",
    "    print(f\"  Generated {len(recommendations_data):,} recommendation records\")\n",
    "    print()\n",
    "    \n",
    "    # Step 4: Save to Parquet\n",
    "    print(f\"[4/4] Saving to {OUTPUT_FILE}...\")\n",
    "    recommendations_df = pd.DataFrame(recommendations_data)\n",
    "    recommendations_df.to_parquet(OUTPUT_FILE, compression='snappy', index=False)\n",
    "    \n",
    "    file_size = os.path.getsize(OUTPUT_FILE) / 1024**2\n",
    "    print(f\"  Saved successfully!\")\n",
    "    print(f\"  File size: {file_size:.2f} MB\")\n",
    "    print(f\"  Compression ratio: {similarity_matrix.nbytes / 1024**2 / file_size:.1f}x\")\n",
    "    print()\n",
    "    \n",
    "    # Verification\n",
    "    print(\"[VERIFICATION] Testing file...\")\n",
    "    test_df = pd.read_parquet(OUTPUT_FILE)\n",
    "    sample_id = movie_indices.iloc[0]['movie_id']\n",
    "    sample_recs = test_df[test_df['movie_id'] == sample_id].head(5)\n",
    "    \n",
    "    print(f\"  Successfully loaded parquet file\")\n",
    "    print(f\"  Sample query (movie_id={sample_id}):\")\n",
    "    print(sample_recs.to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    print(\"\\nRECOMMENDATIONS FILE GENERATED SUCCESSFULLY!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nRecommendations file already exists. Skipping generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Data Loading\n",
    "\n",
    "Verify that the Streamlit app can load data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data loading...\n",
      "================================================================================\n",
      "● Movies loaded successfully\n",
      "  Shape: (16286, 104)\n",
      "  Columns: ['movie_id', 'title', 'original_language', 'release_year', 'runtime']...\n",
      "\n",
      "● Recommendations loaded successfully\n",
      "  Shape: (814300, 4)\n",
      "  Columns: ['movie_id', 'rank', 'recommended_id', 'similarity']\n",
      "\n",
      "● Sample query: 'The Running Man' (ID: 798645)\n",
      "  Found 5 recommendations\n",
      "\n",
      " movie_id  rank  recommended_id  similarity\n",
      "   798645     1             865    0.836517\n",
      "   798645     2          822119    0.816022\n",
      "   798645     3           19959    0.813180\n",
      "   798645     4         1071585    0.798758\n",
      "   798645     5          500664    0.788185\n",
      "\n",
      "● ALL TESTS PASSED! Data is ready for Streamlit app.\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing data loading...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Load movies\n",
    "    movies_df = pd.read_csv(MOVIES_FINAL_FILE)\n",
    "    print(f\"● Movies loaded successfully\")\n",
    "    print(f\"  Shape: {movies_df.shape}\")\n",
    "    print(f\"  Columns: {list(movies_df.columns[:5])}...\")\n",
    "    print()\n",
    "\n",
    "    # Load recommendations\n",
    "    recommendations_df = pd.read_parquet(OUTPUT_FILE)\n",
    "    print(f\"● Recommendations loaded successfully\")\n",
    "    print(f\"  Shape: {recommendations_df.shape}\")\n",
    "    print(f\"  Columns: {list(recommendations_df.columns)}\")\n",
    "    print()\n",
    "\n",
    "    # Test query\n",
    "    sample_movie = movies_df.iloc[0]\n",
    "    sample_movie_id = sample_movie['movie_id']\n",
    "    sample_title = sample_movie['title']\n",
    "\n",
    "    print(f\"● Sample query: '{sample_title}' (ID: {sample_movie_id})\")\n",
    "\n",
    "    sample_recs = recommendations_df[recommendations_df['movie_id'] == sample_movie_id].head(5)\n",
    "    print(f\"  Found {len(sample_recs)} recommendations\")\n",
    "    print()\n",
    "    print(sample_recs.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    print(\"● ALL TESTS PASSED! Data is ready for Streamlit app.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"● Error during testing: {e}\")\n",
    "    print()\n",
    "    print(\"● Please check:\")\n",
    "    print(\"  1. All required files exist\")\n",
    "    print(\"  2. Files are not corrupted\")\n",
    "    print(\"  3. File formats are correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Streamlit App\n",
    "\n",
    "### Option A: Run from Terminal \n",
    "\n",
    "Open your terminal/command prompt and run:\n",
    "\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "The app will open in your browser at `http://localhost:8501`\n",
    "\n",
    "### Option B: Run from Notebook \n",
    "\n",
    "Uncomment and run the cell below to start the app from this notebook.\n",
    "\n",
    "**Note:** The cell will keep running until you stop it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to run Streamlit from notebook\n",
    "# Note: This cell will keep running. Press 'Stop' button to terminate.\n",
    "\n",
    "# !streamlit run app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
